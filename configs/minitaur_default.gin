import safemrl.envs.minitaur
import safemrl.utils.metrics
import safemrl.utils.misc
import safemrl.algos.agents
import tf_agents.environments.suite_pybullet
import tf_agents.networks.actor_distribution_network

EP_LEN = 500
ENV_LOAD_FN = @suite_pybullet.load
ENV_STR = "MinitaurGoalVelocityEnv-v0"
ENV_WRAPPERS = [@minitaur.TaskAgnWrapper, @minitaur.CurrentVelWrapper]
TRAIN_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric(),
    @safemrl.utils.metrics.MinitaurAverageSpeedMetric(),
    @safemrl.utils.metrics.MinitaurAverageMaxSpeedMetric(),
]

# eval_failure/singleton.constructor = @safemrl.utils.metrics.AverageEarlyFailureMetric

EVAL_METRICS = [
    @safemrl.utils.metrics.AverageEarlyFailureMetric(),
    @safemrl.utils.metrics.MinitaurAverageSpeedMetric(),
    @safemrl.utils.metrics.MinitaurAverageMaxSpeedMetric()
]

ENV_METRIC_FACTORIES = []
# safemrl.utils.metrics.AverageEarlyFailureMetric.batch_size = %NUM_ENVS

minitaur.MinitaurGoalVelocityEnv.max_steps = %EP_LEN
safemrl.utils.metrics.AverageEarlyFailureMetric.max_episode_len = %EP_LEN
minitaur.MinitaurGoalVelocityEnv.accurate_motor_model_enabled = True
minitaur.MinitaurGoalVelocityEnv.never_terminate = False
minitaur.MinitaurGoalVelocityEnv.history_length = 5
minitaur.MinitaurGoalVelocityEnv.urdf_version = "rainbow_dash_v0"
minitaur.MinitaurGoalVelocityEnv.history_include_actions = True
minitaur.MinitaurGoalVelocityEnv.control_time_step = 0.02
minitaur.MinitaurGoalVelocityEnv.history_include_states = True
minitaur.MinitaurGoalVelocityEnv.include_leg_model = True

minitaur.MinitaurGoalVelocityEnv.goal_limit = 0.8
minitaur.MinitaurGoalVelocityEnv.goal_vel = 0.4  # 0.4
minitaur.MinitaurGoalVelocityEnv.butterworth = True

actor_distribution_network.ActorDistributionNetwork.preprocessing_combiner = (
    @misc.extract_observation_layer()
)

agents.CriticNetwork.preprocessing_combiner = (
    @misc.extract_obs_merge_w_ac_layer()
)
